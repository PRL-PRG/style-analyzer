---
title: "style-analyzer: The missing artifact"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{R}
library(readr)
library(dplyr)
library(stringr)
library(kableExtra)
library(ggplot2)
library(scales)
library(cowplot)
library(gridExtra)
library(ggpubr)
```

```{r}
# This is where the results of style-analyzer-query live.
# Must have a / at the end.
DJANCO_DIR <- "style-analyzer-query/output/"

# This is where the results of experiments live. 
# Each experiment is a separatre subdir, containing reports and the execution log as .txt (not compressed). 
# The path should not have spaces in it. Must have a / at the end.
REPRO_DIR <- "reproductions/"

# A list of all rperoductions, these are subdirectories of REPRO_DIR. These should not have spaces in them.
# They are used as names, they should not have / in them.
random_projects <- paste0("random_projects_", c(0,2,3,6:9), "_", 10)
random_projects_by_size <- paste0("random_projects_by_size_", c(0:2, 6:9), "_", 10)
EXPERIMENTS <- c("original_reproduction", "original_10", random_projects, random_projects_by_size, "top_starred_projects")

# Convert to bash variables
Sys.setenv(REPRO_DIRS = paste(paste0(REPRO_DIR, "/", EXPERIMENTS), collapse = " "))
```

```{bash}
# We execute this once for every experiment.
for REPRO_DIR in $REPRO_DIRS 
do
  echo "Analyzing $REPRO_DIR"

  # Calculate prediction rates by analyzing the log.
  scripts/extract_prediction_rate.awk \
      <"${REPRO_DIR}/logs.txt" \
      >"${REPRO_DIR}/predictions.csv" 
  
  # Convert summary test report into CSV.
  scripts/extract_summary.awk \
      <"${REPRO_DIR}/summary-test_report.md" \
      >"${REPRO_DIR}/summary.csv"
  
  # Extract unique label counts from individual reports.
  scripts/extract_number_of_labels.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/labels.csv" 
done    
```

```{r}
# Some helper functions
url_to_repo <- function(url) str_replace_all(url, c("(^.*/)|([.]git$)"), "")

# Common: all projects and their locs
all_projects_path <- paste0(DJANCO_DIR, "javascript_projects.csv")
project_locs_path <- paste0(DJANCO_DIR, "project_locs.csv")
  
# Common: from small datatset containing only the projects from the paper
project_locs_path_repro <- paste0(DJANCO_DIR, "repro_project_locs.csv")
all_projects_path_repro <- paste0(DJANCO_DIR, "repro_javascript_projects.csv")

# Load common data.
all_projects <- read_csv(all_projects_path) %>% mutate(repo = url_to_repo(url))
project_locs <- read_csv(project_locs_path) %>% mutate(repo = url_to_repo(url))

# Repro extras: attach the data from small dataset to the big dataset
all_projects_repro <- read_csv(all_projects_path_repro) %>% mutate(repo = url_to_repo(url))
project_locs_repro <- read_csv(project_locs_path_repro) %>% mutate(repo = url_to_repo(url))
all_projects <- bind_rows(all_projects, all_projects_repro) %>% distinct
project_locs <- bind_rows(project_locs, project_locs_repro) %>% distinct

# Construct a table with the results of one experiment
prepare_data <- function(experiment) {
  cat(paste0("Preparing results for ", experiment))
  
  # Paths
  predictions_path <- paste0(REPRO_DIR, experiment, "/predictions.csv")
  summary_path <- paste0(REPRO_DIR, experiment, "/summary.csv")
  labels_path <- paste0(REPRO_DIR, experiment, "/labels.csv")

  # CSVs
  predictions <- read_csv(predictions_path) # there is one in project_locs and this one has a different format
  summary <- read_csv(summary_path) %>% mutate(repo = url_to_repo(repo))
  labels <- read_csv(labels_path) %>% mutate(repo = url_to_repo(repo))
  
  # Collate results 
  collated <- predictions %>%
    left_join(project_locs, by=c("url", "repo")) %>%
    left_join(summary, by="repo") %>%
    left_join(labels, by="repo") %>%
    filter(!is.na(precision))
    
  # Format the output
  collated %>%
    select(-recall, -f1) %>%
    mutate(pred_r=predictions/samples) %>%
    rename(recall=full_recall, 
           f1=full_f1,
           suppport=full_support,
           avg_rule_len=`Average Rule Len`,
           rules=`Rules Number`) %>%
    select(url, repo, precision, pred_r, 
           recall, f1, support, locs,
           labels, rules, avg_rule_len,  
           training_time) %>%
    distinct
}

# Prepare data for all experiments. Put the results in a list.
experiment_data <- lapply(EXPERIMENTS, prepare_data)
names(experiment_data) <- EXPERIMENTS

for (experiment in experiment_data) {
  print(experiment)
}
```

See what finished processing:

```{r}
status_table <- tibble(experiment=character(0), url=character(0), locs=integer(0), done=logical(0))
for (experiment in EXPERIMENTS) {
  query_result_path <- paste0(DJANCO_DIR, experiment, ".csv")
  if (!file.exists(query_result_path)) {
    warning("Could not find file: ", query_result_path, ". Skipping.\n")
  } else {
    query_result <- read_csv(query_result_path) %>% select(url)
    processed_projects <- experiment_data[[experiment]] %>% select(url)
    processed_projects_summary <- query_result %>% 
      mutate(experiment=experiment) %>% 
      left_join(processed_projects %>% mutate(done=TRUE), by="url") %>%
      left_join(project_locs, by="url")
    #print(processed_projects_summary)
    status_table <- bind_rows(status_table, processed_projects_summary %>% select(experiment, url, locs, done))
  }
}
status_table
```

```{r}
summary_status_table <- status_table %>% 
  mutate(completed=ifelse(is.na(done), 0, ifelse(done, 1, 0))) %>% 
  group_by(experiment) %>% 
  summarize(completed=sum(completed), 
            percent_completed=(completed*100)/n(), 
            failed=n()-completed, 
            percent_failed=(100*failed)/n()) %>% 
  ungroup

summary_status_table
```

```{r}
summary_status_table %>% 
  summarize(projects_in_all_datasets=sum(completed) + sum(failed), 
            completed=sum(completed), 
            percent_completed=format((completed*100)/projects_in_all_datasets, digits=1), 
            failed=sum(failed), 
            percent_failed=format((100*failed)/projects_in_all_datasets, digits=1))
```

```{r}
classify_by_size <- function(n) {
    ifelse(n >= 10**5, "very big", 
    ifelse(n >= 10**4, "large",
    ifelse(n >= 10**3, "medium",
                       "small")))
  }

status_by_size <- status_table %>% 
  mutate(size=classify_by_size(locs)) %>% 
  group_by(size) %>%
  mutate() %>%
  ungroup
  


```

## Metrics measured on the validation part of the dataset. 

The last row is weighed by the number of samples.

```{r}
make_paper_table <- function(experiment, latex = FALSE, digits=3) {
  
  data <- experiment_data[[experiment]] %>% mutate(`training time, min` = round(training_time/60))
  
  stringified <-
    data %>% 
    mutate(repo = repo, 
           precision = format(precision, digits=digits),
           pred_r = format(pred_r, digits=digits),
           recall = format(recall, digits=digits),
           f1 = format(f1, digits=digits),
           support = format(support, digits=digits),
           locs = format(locs, digits=digits),
           rules = format(rules, digits=digits),
           avg_rule_len = format(avg_rule_len, digits=digits),
           labels = format(labels, digits=digits),
           training_time = format(training_time, digits=digits)) %>%
    arrange(desc(support))

  average <- 
    data %>% 
    summarize(repo = "average", 
              precision = format(mean(precision), digits=digits),
              pred_r = format(mean(pred_r), digits=digits),
              recall = format(mean(recall), digits=digits),
              f1 = format(mean(f1), digits=digits),
              support = format(mean(support), digits=digits),
              locs = NA,
              rules = format(mean(rules), digits=digits),
              avg_rule_len = format(mean(avg_rule_len), digits=digits),
              labels = format(mean(labels), digits=digits),
              training_time = NA)
  
  weighted_average <- 
    data %>% 
    summarize(repo = "weighted average", 
              precision = format(weighted.mean(precision, support), digits=digits),
              pred_r = format(weighted.mean(pred_r, support),  digits=digits),
              recall = format(weighted.mean(recall, support), digits=digits),
              f1 = format(weighted.mean(f1, support), digits=digits),
              locs = NA,
              rules = NA,
              avg_rule_len = NA,
              labels = NA,
              training_time = NA,
              support = NA)
  
  
  
  table <- bind_rows(stringified, average, weighted_average) %>%
    rename(repository=repo,
           `PredR`=pred_r,
           `train samples` = support,
           `LoC` = locs,
           `unique labels` = labels,
           `avg. rule length` = avg_rule_len) %>%
    select(`repository`, `precision`, `PredR`, 
           `recall`, `f1`, `train samples`, `LoC`, 
           `unique labels`, `rules`, `avg. rule length`, 
           `training time, min`)
  
  k <- kable(table, booktabs = TRUE, format = ifelse(latex, "latex", "html"), caption = paste0("Selection: ", experiment)) %>%
    kable_styling(latex_options = "striped") %>%
    row_spec((table %>% count)$n - 3, hline_after = TRUE) # Add hline before average and weighted average
  
  return(k)
}
```

```{r}
#for (experiment in EXPERIMENTS) {
#  print(make_paper_table(experiment, TRUE))
#}
```

```{r}
make_paper_table("original_reproduction")
```

```{r}
make_paper_table("original_10")
```

```{r}
for (experiment in random_projects) {
  print(make_paper_table(experiment))
}
```

```{r}
for (experiment in random_projects_by_size) {
  print(make_paper_table(experiment))
}
```

```{r}
for (experiment in EXPERIMENTS) {
  print(make_paper_table(experiment))
}
```

## Effect of the number of samples in the training set on precision

```{r}
make_precision_support_graph <- function (experiment) {
  data <- experiment_data[[experiment]]
  ggplot(data, aes(x=precision, y=support)) +
    geom_point(size=2) + 
    geom_text(aes(label=repo), position=position_nudge(y=-5000),  size=3) +
    scale_x_continuous(labels=function(x) format(x, digits=2)) +
    scale_y_continuous(labels=label_number_si()) +
    xlab("Precision") + 
    ylab("Samples in the training set") +
    ggtitle(paste0("Selection: ", experiment)) +
    theme_bw()
}
```

```{r}
make_precision_support_graph("original_reproduction")
```

```{r}
make_precision_support_graph("original_10")
```


```{r}
for (experiment in random_projects) {
  print(make_precision_support_graph(experiment))
}
```

```{r}
for (experiment in random_projects_by_size) {
  print(make_precision_support_graph(experiment))
}
```

```{r}
make_precision_support_graph("top_starred_projects")
```

# Notes

## Size analysis

```{r}
ggplot(project_locs, aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=1000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_locs %>% filter(locs < 100000), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=1000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (only 50K or less)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
project_size <- project_locs %>% 
  mutate(size = ifelse(locs > 10**5, "very large", 
                ifelse(locs > 10**4, "large", 
                ifelse(locs > 10**3, "medium", "small"))))

ggplot(project_size, aes(x=size)) +
    geom_histogram(stat="count") +
    xlab("Project size") +
    ylab("# projects") +
    scale_x_discrete() +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "very large"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=10000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (very large projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "large"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=1000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (large projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "medium"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=100,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (medium projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "small"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=10,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (small projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```
