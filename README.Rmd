---
title: "style-analyzer: reproduction artifact"
output: html_document
author: Konrad Siek
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```bash
sudo apt update

# I don't know if we actually need these, probably not.
sudo apt install python3 python3-pip
pip3 install pip install git+git://github.com/snowballstem/pystemmer
pip3 install bblfsh

# Docker
sudo curl -sSL https://get.docker.com/ | sh
sudo chmod 666 /var/run/docker.sock

# Rust and djanco
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
cargo install --git https://github.com/PRL-PRG/cargo-djanco

# Start bblfsh
make bblfsh-start
```

# Generate selections

```bash
cd style-selection-query

# Prepare the query
cargo djanco

# Run for reproduction dataset
cargo run --bin djanco --release -- --output-path output/repro \
 --dataset-path /data/parasite/style-analyzer-merged/ \
 --cache-path /data/parasite/style-analyzer-merged-cache/

# Run for big dataset
cargo run --bin djanco --release -- \
  --output-path output --dataset-path /mnt/data/parasite/style-analyzer-merged/ \
  --cache-path /data/parasite/style-analyzer-merged-cache/

# Combine
cp output/repro/info/javascript_projects.csv \
   output/selections/repro_javascript_projects.csv
cp output/repro/info/project_locs.csv \
   output/selections/repro_project_locs.csv

cd ..
```

# Process selections

```bash
make run-experiment
```

(To run just one or a few selections: `make run-experiment selections="x y z"`)

Probably fix the access flags for the `reproductions` dir

```bash
sudo chmod -R a+rw reproductions
```

# Experiment

```{R}
library(readr)
library(dplyr)
library(stringr)
library(kableExtra)
library(ggplot2)
library(scales)
library(cowplot)
library(gridExtra)
library(ggpubr)
```

```{r}
# This is where the results of style-analyzer-query live.
# Must have a / at the end.
DJANCO_DIR <- "style-analyzer-query/output/"

# This is where the results of experiments live. 
# Each experiment is a separatre subdir, containing reports and the execution log as .txt (not compressed). 
# The path should not have spaces in it. Must have a / at the end.
REPRO_DIR <- "reproductions/"

# A list of all rperoductions, these are subdirectories of REPRO_DIR. These should not have spaces in them.
# They are used as names, they should not have / in them.
random_projects <- paste0("random_projects_", c(0,2,3,6:9), "_", 10)
random_projects_by_size <- paste0("random_projects_by_size_", c(0:2, 6:9), "_", 10)
EXPERIMENTS <- c("original_reproduction", "original_10", random_projects, random_projects_by_size, "top_starred_projects", "quality_projects_10")

# Convert to bash variables
Sys.setenv(REPRO_DIRS = paste(paste0(REPRO_DIR, "/", EXPERIMENTS), collapse = " "))
```

```{bash}
# We execute this once for every experiment.
for REPRO_DIR in $REPRO_DIRS 
do
  echo "Analyzing $REPRO_DIR"

  # Calculate prediction rates by analyzing the log.
  scripts/extract_prediction_rate.awk \
      <"${REPRO_DIR}/logs.txt" \
      >"${REPRO_DIR}/predictions.csv" 
  
  # Convert summary test report into CSV.
  scripts/extract_summary.awk \
      <"${REPRO_DIR}/summary-test_report.md" \
      >"${REPRO_DIR}/summary.csv" \
  || echo "Failed to generate summary.csv for ${REPRO_DIR}: this means the selection didn't finish processing." \
          "It only matters if you want to generate the big table."
  
  # Extract unique label counts from individual reports.
  scripts/extract_number_of_labels.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/labels.csv" 
      
  # Extract support and full support unmbers for training data.
  scripts/extract_training_samples.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/training_samples.csv" 
      
  # Extract support and full support unmbers for training data.
  scripts/extract_test_precision.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/precision.csv" 
done    
```

```{r}
# Some helper functions
url_to_repo <- function(url) str_replace_all(url, c("(^.*/)|([.]git$)"), "")

# Common: all projects and their locs
all_projects_path <- paste0(DJANCO_DIR, "info/javascript_projects.csv")
project_locs_path <- paste0(DJANCO_DIR, "info/project_locs.csv")
  
# Common: from small datatset containing only the projects from the paper
project_locs_path_repro <- paste0(DJANCO_DIR, "info/repro_project_locs.csv")
all_projects_path_repro <- paste0(DJANCO_DIR, "info/repro_javascript_projects.csv")

# Load common data.
all_projects <- read_csv(all_projects_path) %>% mutate(repo = url_to_repo(url))
project_locs <- read_csv(project_locs_path) %>% mutate(repo = url_to_repo(url))

# Repro extras: attach the data from small dataset to the big dataset
all_projects_repro <- read_csv(all_projects_path_repro) %>% mutate(repo = url_to_repo(url))
project_locs_repro <- read_csv(project_locs_path_repro) %>% mutate(repo = url_to_repo(url))
all_projects <- bind_rows(all_projects, all_projects_repro) %>% distinct
project_locs <- bind_rows(project_locs, project_locs_repro) %>% distinct

# Construct a table with the results of one experiment
prepare_data <- function(experiment) {
  cat(paste0("Preparing results for ", experiment))
  
  # Paths
  predictions_path <- paste0(REPRO_DIR, experiment, "/predictions.csv")
  summary_path <- paste0(REPRO_DIR, experiment, "/summary.csv")
  labels_path <- paste0(REPRO_DIR, experiment, "/labels.csv")
  samples_path <- paste0(REPRO_DIR, experiment, "/training_samples.csv")
  precision_path <- paste0(REPRO_DIR, experiment, "/precision.csv")

  # CSVs
  predictions <- read_csv(predictions_path) # there is one in project_locs and this one has a different format
  labels <- read_csv(labels_path) %>% mutate(repo = url_to_repo(repo))
  samples <- read_csv(samples_path) %>% mutate(repo = url_to_repo(repo)) %>% rename(report_training_samples=support, report_full_training_samples=full_support)
  precision <- read_csv(precision_path) %>% mutate(repo = url_to_repo(repo)) %>% rename(report_precision=precision)
  
  # Summary is optional, since we're concentrating on graphs not tables.
  # If the whole selection didn't finish processing, the summary won't be generated, 
  # but everything else will be.
  finished_processing <- file.exists(summary_path);
  summary <-
    if (finished_processing) read_csv(summary_path) %>% mutate(repo = url_to_repo(repo))
    else tibble(repo=character(0), precision=numeric(0), 
                recall=numeric(0), full_recall=numeric(0), 
                f1=numeric(0), full_f1=numeric(0), 
                ppcr=integer(0), 
                support=integer(0), full_support=integer(0), 
                `Rules Number`=integer(0), `Average Rule Len`=numeric(0))
  
  # Collate results 
  collated <- predictions %>%
    left_join(project_locs, by=c("url", "repo")) %>%
    left_join(summary, by="repo") %>%
    left_join(labels, by="repo") %>%
    left_join(samples, by="repo") %>%
    left_join(precision, by="repo")
   
    
  # Format the output
  collated %>%
    select(-recall, -f1, -support, -precision) %>%
    mutate(pred_r=predictions/samples) %>%
    rename(recall=full_recall, 
           precision=report_precision,
           f1=full_f1,
           support=report_training_samples,
           avg_rule_len=`Average Rule Len`,
           rules=`Rules Number`) %>%
    select(url, repo, precision, pred_r, 
           recall, f1, support, locs,
           labels, rules, avg_rule_len,  
           training_time) %>%
    filter(!is.na(precision)) %>%
    distinct 
}

# Prepare data for all experiments. Put the results in a list.
experiment_data <- lapply(EXPERIMENTS, prepare_data)
names(experiment_data) <- EXPERIMENTS

experiment_data[["paper"]] <- tibble(
  repo       = c("node", "webpack", "meteor", "react", "atom", "react-native", "jquery", "storybook", "freeCodeCamp", "express", "30-seconds-of-code", "evergreen", "citgm", "axios", "create-react-app", "redux", "reveal.js", "carlo", "telescope"),
  precision  = c(0.965,  0.957,     0.9,      0.943,   0.955,   0.94,          0.972,    0.94,        0.928,           0.937,    0.951,                0.894,       0.936,    0.94,   0.895,              0.937,    0.897,      0.878,   0.806),
  support    = c(374298, 358012,    337627,   304465,  265521,  264961,        197072,   161366,      114020,          78411,    67737,                38387,       21941,    21130,  16718,              14783,    9974,       5529,    731)
)

for (experiment in experiment_data) {
  print(experiment)
}
```

See what finished processing:

```{r}
status_table <- tibble(experiment=character(0), url=character(0), locs=integer(0), done=logical(0))
for (experiment in EXPERIMENTS) {
  query_result_path <- paste0(DJANCO_DIR, experiment, ".csv")
  if (!file.exists(query_result_path)) {
    warning("Could not find file: ", query_result_path, ". Skipping.\n")
  } else {
    query_result <- read_csv(query_result_path) %>% select(url)
    #print(paste0("WTF qr", experiment, "->", query_result  %>% count() %>% pull(n)))
    processed_projects <- experiment_data[[experiment]] %>% select(url)
    processed_projects_summary <- query_result %>% 
      mutate(experiment=experiment) %>% 
      left_join(processed_projects %>% mutate(done=TRUE), by="url") %>%
      left_join(project_locs, by="url")
    #print(processed_projects_summary)
    #print(paste0("WTF ", experiment, "->", processed_projects_summary  %>% count() %>% pull(n)))
    status_table <- bind_rows(status_table, processed_projects_summary %>% select(experiment, url, locs, done))
  }
}
status_table
```

```{r}
summary_status_table <- status_table %>% 
  mutate(completed=ifelse(is.na(done), 0, ifelse(done, 1, 0))) %>% 
  group_by(experiment) %>% 
  summarize(completed=sum(completed), 
            percent_completed=(completed*100)/n(), 
            failed=n()-completed, 
            percent_failed=(100*failed)/n()) %>% 
  ungroup

summary_status_table
```

```{r}
summary_status_table %>% 
  summarize(projects_in_all_datasets=sum(completed) + sum(failed), 
            completed=sum(completed), 
            percent_completed=format((completed*100)/projects_in_all_datasets, digits=1), 
            failed=sum(failed), 
            percent_failed=format((100*failed)/projects_in_all_datasets, digits=1))
```

```{r}
classify_by_size <- function(n) {
    ifelse(n >= 10**5, "very large", 
    ifelse(n >= 10**4, "large",
    ifelse(n >= 10**3, "medium",
                       "small")))
  }

status_by_size <- status_table %>% 
  mutate(size=classify_by_size(locs)) %>% 
  mutate(completed=ifelse(is.na(done), 0, ifelse(done, 1, 0))) %>% 
  group_by(size) %>%
  summarise(projects=n(), 
            completed=sum(completed), 
            percent_completed=format((completed*100)/projects, digits=1), 
            failed=projects-completed,
            percent_failed=format((failed*100)/projects, digits=1)) %>%
  ungroup
  
status_by_size

```

## Making them big

```{r}
connect_selections <- function(selections, label) {
  random_data <- NULL
  for (random in random_projects) {
    if (is.null(random_data)) {
      random_data <- experiment_data[[random]] %>% mutate(selection = random)
    } else {
      random_data <- bind_rows(random_data, experiment_data[[random]] %>% mutate(selection = random))
    }
  }
  experiment_data[[label]] <<- random_data %>% distinct
  EXPERIMENTS <<- c(EXPERIMENTS, label)
  experiment_data[[label]]
}

connect_selections(random_projects, "random_projects_all_10")
connect_selections(random_projects_by_size, "random_projects_by_size_all_10")
```

## Metrics measured on the validation part of the dataset. 

The last row is weighed by the number of samples.

```{r}
make_paper_table <- function(experiment, latex = FALSE, digits=3) {
  
  data <- experiment_data[[experiment]] %>% mutate(`training time, min` = round(training_time/60))
  
  stringified <-
    data %>% 
    mutate(repo = repo, 
           precision = format(precision, digits=digits),
           pred_r = format(pred_r, digits=digits),
           recall = format(recall, digits=digits),
           f1 = format(f1, digits=digits),
           support = format(support, digits=digits),
           locs = format(locs, digits=digits),
           rules = format(rules, digits=digits),
           avg_rule_len = format(avg_rule_len, digits=digits),
           labels = format(labels, digits=digits),
           training_time = format(training_time, digits=digits)) %>%
    arrange(desc(support))

  average <- 
    data %>% 
    summarize(repo = "average", 
              precision = format(mean(precision), digits=digits),
              pred_r = format(mean(pred_r), digits=digits),
              recall = format(mean(recall), digits=digits),
              f1 = format(mean(f1), digits=digits),
              support = format(mean(support), digits=digits),
              locs = NA,
              rules = format(mean(rules), digits=digits),
              avg_rule_len = format(mean(avg_rule_len), digits=digits),
              labels = format(mean(labels), digits=digits),
              training_time = NA)
  
  weighted_average <- 
    data %>% 
    summarize(repo = "weighted average", 
              precision = format(weighted.mean(precision, support), digits=digits),
              pred_r = format(weighted.mean(pred_r, support),  digits=digits),
              recall = format(weighted.mean(recall, support), digits=digits),
              f1 = format(weighted.mean(f1, support), digits=digits),
              locs = NA,
              rules = NA,
              avg_rule_len = NA,
              labels = NA,
              training_time = NA,
              support = NA)
  
  
  
  table <- bind_rows(stringified, average, weighted_average) %>%
    rename(repository=repo,
           `PredR`=pred_r,
           `train samples` = support,
           `LoC` = locs,
           `unique labels` = labels,
           `avg. rule length` = avg_rule_len) %>%
    select(`repository`, `precision`, `PredR`, 
           `recall`, `f1`, `train samples`, `LoC`, 
           `unique labels`, `rules`, `avg. rule length`, 
           `training time, min`)
  
  k <- kable(table, booktabs = TRUE, format = ifelse(latex, "latex", "html"), caption = paste0("Selection: ", experiment)) %>%
    kable_styling(latex_options = "striped") %>%
    row_spec((table %>% count)$n - 3, hline_after = TRUE) # Add hline before average and weighted average
  
  return(k)
}
```

```{r}
#for (experiment in EXPERIMENTS) {
#  print(make_paper_table(experiment, TRUE))
#}
```

```{r}
make_paper_table("original_reproduction")
```

```{r}
make_paper_table("original_10")
```

```{r}
make_paper_table("random_projects_all_10")
```

```{r}
for (experiment in random_projects) {
  print(make_paper_table(experiment))
}
```

```{r}
make_paper_table("random_projects_by_size_all_10")
```

```{r}
for (experiment in random_projects_by_size) {
  print(make_paper_table(experiment))
}
```

```{r}
for (experiment in EXPERIMENTS) {
  print(make_paper_table(experiment))
}
```

## Effect of the number of samples in the training set on precision

```{r}
min_precision = sapply(experiment_data, function(data) min(data$precision)) %>% min
max_support = sapply(experiment_data, function(data) max(data$support)) %>% max
make_precision_support_graph <- function (experiment, uniform_scales=TRUE, labels=FALSE, color_by_selection=FALSE, log_scale=FALSE, only_top_n=NULL,  only_bottom_n=NULL) {
  data <- experiment_data[[experiment]]
  if (!is.null(only_top_n)) {
    data <- data %>% top_n(only_top_n)
  }
  if (!is.null(only_bottom_n)) {
    data <- data %>% top_n(-only_bottom_n)
  }
  plot <- 
  if (color_by_selection) {
    ggplot(data, aes(x=precision, y=support, alpha=0.2, color=selection)) + geom_point(size=2)
  } else {
    ggplot(data, aes(x=precision, y=support, alpha=0.2)) + geom_point(size=2)
  }
  if (labels) {
    plot <- plot + geom_text(aes(label=repo), position=position_nudge(y=-10000),  size=3)
  }
  
  x_range <- if(uniform_scales) c(min_precision, 1) else NULL
  y_range <- if(uniform_scales) c(0 - 10000, max_support) else NULL
  
  if (log_scale) {
    plot <- plot + scale_y_log10(labels=label_number_si(), limits = y_range)
  } else {
    plot <- plot + scale_y_continuous(labels=label_number_si(), limits = y_range)
  }
  
  plot +
    scale_x_continuous(labels=function(x) format(x, digits=2), limits = x_range) +
    xlab("Precision") + 
    ylab("Samples in the training set") +
    ggtitle(paste0("Selection: ", experiment)) +
    theme_bw() + 
    theme(legend.position = "none")
}
```

## Reproduction vs original

```{r}
make_precision_support_graph("paper")
```

```{r}
make_precision_support_graph("original_reproduction")
```

```{r}
make_precision_support_graph("original_10")
```

```{r}
make_precision_support_graph("top_starred_projects")
```

```{r}
comparison <- inner_join(experiment_data$paper, experiment_data$original_reproduction, by="repo")
ggplot() + 
  geom_segment(comparison, mapping=aes(x=precision.x, xend=precision.y, y=support.x, yend=support.y, alpha=0.2, color=repo), arrow=arrow()) + 
  #geom_text(data=comparison, aes(x=precision.x, y=support.x, label=repo)) + 
  theme_bw() +  
  scale_x_continuous(labels=function(x) format(x, digits=2), limits = c(min_precision, 1)) +
  scale_y_log10(labels=label_number_si(), limits =  c(0 - 10000, max_support)) +
  ggtitle("Paper vs reproduction using input from artifact") +
  xlab("Precision") +
  ylab("Samples in training set") +
  theme(legend.position="none")
```

```{r, fig.height=12, fig.width=12}
ggplot() + 
  geom_segment(comparison, mapping=aes(x=precision.x, xend=precision.y, y=support.x, yend=support.y, alpha=0.2, color=repo), arrow=arrow()) + 
  geom_text(data=comparison, aes(x=precision.x, y=support.x, label=repo)) + 
  theme_bw() +  
  scale_x_continuous(labels=function(x) format(x, digits=2)) + #, limits = c(min_precision, 1)) +
  scale_y_log10(labels=label_number_si()) + #limits =  c(0 - 10000, max_support)) +
  ggtitle("Paper vs reproduction using input from artifact") + # (same, just bigger and not scaled like everything else)") +
  xlab("Precision") +
  ylab("Samples in training set") +
  theme(legend.position="none")
```

How much more precision/support do we get for the reproduction:

```{r}
comparison %>% 
  mutate(precision=precision.y,
         precision_delta=precision.y-precision.x, 
         precision_delta_perc=format(100*precision_delta/precision.x, digits=1),
         support=support.y,
         support_delta=support.y-support.x,
         support_delta_perc=format(100*support_delta/support.x, digits=1)) %>% 
  select(repo, precision, precision_delta, precision_delta_perc, support, support_delta,  support_delta_perc)
```

### Quality projects vs original

```{r}
make_precision_support_graph("original_10")
```

```{r}
make_precision_support_graph("quality_projects_10", only_top_n=20)
```

```{r}
make_precision_support_graph("quality_projects_10", only_bottom_n=20)
```

```{r}
strip_and_label <- function(data, label) data %>% mutate(selection=label) %>% select(selection, repo, precision, support)

comparison_data <- 
  experiment_data[["original_10"]] %>% strip_and_label("original") %>%
  bind_rows(experiment_data[["quality_projects_10"]] %>% top_n(20) %>% strip_and_label("quality_projects_1")) %>%
  bind_rows(experiment_data[["quality_projects_10"]] %>% top_n(-20) %>% strip_and_label("quality_projects_2"))

ggplot(comparison_data, aes(x=selection, y=support)) + geom_boxplot()
```

### Random samples

```{r}
make_precision_support_graph("original_reproduction")
```

```{r}
make_precision_support_graph("original_10")
```

```{r}
make_precision_support_graph("random_projects_all_10", color_by_selection = TRUE)
```

```{r}
for (experiment in random_projects) {
  print(make_precision_support_graph(experiment))
}
```

```{r}
make_precision_support_graph("random_projects_by_size_all_10", color_by_selection = TRUE)
```

```{r}
for (experiment in random_projects_by_size) {
  print(make_precision_support_graph(experiment))
}
```

# Notes

## Size analysis

```{r}
ggplot(project_locs, aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=1000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_locs %>% filter(locs < 100000), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=1000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (only 50K or less)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
project_size <- project_locs %>% 
  mutate(size = ifelse(locs > 10**5, "very large", 
                ifelse(locs > 10**4, "large", 
                ifelse(locs > 10**3, "medium", "small"))))

ggplot(project_size, aes(x=size)) +
    geom_histogram(stat="count") +
    xlab("Project size") +
    ylab("# projects") +
    scale_x_discrete() +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "very large"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=10000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (very large projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "large"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=1000,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (large projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "medium"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=100,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (medium projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```

```{r}
ggplot(project_size %>% filter(size == "small"), aes(x=locs)) +
    geom_histogram(aes(y=..density..),
                   binwidth=10,
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666") +
    xlab("Lines of code in head of default branch (small projects)") +
    ylab("# projects") +
    scale_x_continuous(labels=label_number_si()) +
    scale_y_continuous(labels=label_number_si()) +
    theme_bw()
```
